{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training takes 8.942448139190674s seconds\n",
      "SVD takes 0.15140604972839355s seconds\n",
      "EDC Matrix calculation takes 0.021645545959472656s seconds\n",
      "MADC Matrix calculation takes 0.021036386489868164s seconds\n",
      "Clustering takes 0.015588760375976562s seconds\n",
      "Clustering Results: Counter({0: 32, 1: 15, 2: 13})\n",
      "---------- Round 0 ----------\n",
      "Round 0, Group 0 has 10 client.\n",
      "Round 0, Group 1 has 7 client.\n",
      "Round 0, Group 2 has 3 client.\n",
      "groupbase.py Test NKS: [121, 60, 108]\n",
      "\u001b[7m\u001b[31mRound 0, Test ACC: 0.7163,            Test Loss: 1.2564\u001b[0m\n",
      "Round 0, Group: 2, Test ACC: 0.8678,                    Test Loss: 0.697\n",
      "Round 0, Group: 1, Test ACC: 0.8833,                    Test Loss: 0.6882\n",
      "Round 0, Group: 0, Test ACC: 0.4537,                    Test Loss: 2.1989\n",
      "groupbase.py Auxiliary Model Test NKS: [7371]\n",
      "\u001b[7m\u001b[32mRound 0, Auxiliary Model Test ACC: 0.2877,            Auxiliary Model Test Loss: 2.4265\u001b[0m\n",
      "groupbase.py Train NKS: [103.0, 200.0, 189.0]\n",
      "\u001b[7m\u001b[34mRound 0, Train ACC: 1.0,            Train Loss: 0.2735\u001b[0m\n",
      "Round 0, Group: 2, Train ACC: 1.0,                    Train Loss: 0.2653\n",
      "Round 0, Group: 1, Train ACC: 1.0,                    Train Loss: 0.2688\n",
      "Round 0, Group: 0, Train ACC: 1.0,                    Train Loss: 0.2831\n",
      "Round: 0, Training time: 5.007, Test time: 0.593,                 Inter-Group Aggregate time: 0.009\n",
      "---------- Round 1 ----------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from utils import test_trainer\n",
    "#import importlib\n",
    "\n",
    "print('This is a debug file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# Input Layer\n",
    "model.add(Input(shape=(784,)))\n",
    "# Reshape Layer\n",
    "model.add(Reshape((28, 28, 1)))\n",
    "# Conv Layer\n",
    "model.add(Conv2D(24, 5, padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
    "# MaxPool Layer\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# Flatten Layer\n",
    "model.add(Flatten())\n",
    "# Dense Layer\n",
    "model.add(Dense(256, 'relu'))\n",
    "# Output Layer\n",
    "model.add(Dense(10, 'softmax'))\n",
    "\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(opt, loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# Input Layer\n",
    "model.add(Input(shape=(784,)))\n",
    "# Output Layer\n",
    "model.add(Dense(10, 'softmax', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    \n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.003)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(opt, loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.get_weights()\n",
    "print(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "random_idx = np.arange(5)\n",
    "print(random_idx[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'x':[],'y':[]}\n",
    "b = {'x':[],'y':[]}\n",
    "if a is b:\n",
    "    print('TRUE')\n",
    "else:\n",
    "    print('FALSE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
